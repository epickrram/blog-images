<h1 id="navigating-the-linux-kernel-network-stack-into-user-land">Navigating the Linux kernel network stack: into user land</h1>
<p>This is a continuation of my <a href="http://epickrram.blogspot.co.uk/2016/05/navigating-linux-kernel-network-stack.html">previous post</a> in which we follow the path of an inbound multicast packet to the user application.</p>
<p>At the end of the last post, I mentioned that application back-pressure was likely to be the cause of receiver packet-loss. As we continue through the code path taken by inbound packets up into user-land, we will see the various points at which data will be discarded due to slow application processing, and what metrics can shed light on these events.</p>
<h2 id="protocol-mapping">Protocol mapping</h2>
<p>First of all, let's take a quick look at how a received data packet is matched up to its <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L1735">handler function</a>.</p>
<p>We can see from the stack-trace below that the top-level function calls when dealing with the packets are <code>packet_rcv</code> and <code>ip_rcv</code>.</p>
<pre><code>__netif_receive_skb_core() {
    packet_rcv() {
        skb_push();
        __bpf_prog_run();
        consume_skb();
    }
    bond_handle_frame() {
        bond_3ad_lacpdu_recv();
    }
    packet_rcv() {
        skb_push();
        __bpf_prog_run();
        consume_skb();
    }
    packet_rcv() {
        skb_push();
        __bpf_prog_run();
        consume_skb();
    }
    ip_rcv() {
        ...</code></pre>
<p>This trace captures the <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L3671">part of the receive path</a> where the inbound packet is passed to each registered handler function. In this way, the kernel handles things like VLAN tagging, interface bonding, and packet-capture. Note the <code>__bpf_prog_run</code> function call, which indicates that this is the entry point for <code>pcap</code> packet capture and filtering.</p>
<p>The protocol-to-handler mapping can be viewed in the file <code>/proc/net/ptype</code>:</p>
<pre><code>[pricem@metal]$ cat /proc/net/ptype 
Type Device      Function
ALL  em1      packet_rcv
0800          ip_rcv
0011          llc_rcv [llc]
0004          llc_rcv [llc]
88f5          mrp_rcv [mrp]
0806          arp_rcv
86dd          ipv6_rcv</code></pre>
<p>Comparing against <a href="https://en.wikipedia.org/wiki/EtherType#Notable_values">this reference</a>, it is clear that the kernel reads the value of the ethernet frame's <code>EtherType</code> field and dispatches to the corresponding function. There are also some functions that will be executed for all packet types (signified by type <code>ALL</code>).</p>
<p>So our inbound packet will be processed by the capture hooks, even if no packet capture is running (this then must presumably be very cheap), before being passed to the correct protocol handler, in this case <code>ip_rcv</code>.</p>
<h2 id="the-socket-buffer">The socket buffer</h2>
<p>As discussed previously, each socket on the system has a receive-side FIFO queue that is written to by the kernel, and read from by the user application.</p>
<p><a href="http://lxr.free-electrons.com/source/net/ipv4/ip_input.c?v=4.0#L376"><code>ip_rcv</code></a> starts by getting its own copy of the incoming packet, copying if the packet is already shared. If the copy fails due to lack of memory, then the packet is discarded, and the <code>Discard</code> count of ipstats is incremented. Other checks made at this point include the IP checksum, header-length check and truncation check, each of which update the relevant metrics in the ipstats table.</p>
<p>Before calling the <code>ip_rcv_finish</code> function, the packet is diverted through the <code>netfilter</code> module where software-based network filtering can be applied.</p>
<p>Assuming that the packet is not dropped by a filter, <a href="http://lxr.free-electrons.com/source/net/ipv4/ip_input.c?v=4.0#L312"><code>ip_rcv_finish</code></a> passes the packet on to the next protocol-handler in the chain.</p>
<p>In the internals of the <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1749"><code>udp_rcv</code></a> function, we finally get to the point of accessing the socket FIFO queue.</p>
<p>Simple validation performed at this point includes packet-length check and checksum. Failure of these checks will cause the relevant statistics to be updated in the udpstats table.</p>
<p>Because the traffic we're tracing <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1801">is multicast</a>, the next handler function is <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1660"><code>__udp4_lib_mcast_deliver</code></a>.</p>
<p>With some exotic locking in place, the kernel determines how many different sockets this multicast packet needs to be delivered to. It is worth noting here that there is a smallish number (<code>256 / sizeof( struct sock)</code>) of sockets that can be listening to a given multicast address before the possibility of greater lock-contention creeps in.</p>
<p>An effort is made to enumerate the registered sockets with a lock held, then perform the packet dispatching without a lock. However, if the number of registered sockets exceeds the specified threshold, then packet dispatch (the <code>flush_stack</code> function) will be handled while a lock is held.</p>
<p>Once in <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1614"><code>flush_stack</code></a>, the packet is copied for each registered socket, and pushed onto the FIFO queue.</p>
<p>If the kernel is unable to allocate memory in order to copy the buffer, the socket's drop-count will be incremented, along with the RcvBufErrors and InErrors metrics in the udpstats table.</p>
<p>After another checksum test, we are finally at the point where socket-buffer overflow <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1584">is tested</a>:</p>
<pre><code>if (sk_rcvqueues_full(sk, sk-&gt;sk_rcvbuf)) {
    UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,
                     is_udplite);
    goto drop;
}</code></pre>
<p>If the size of the socket's backlog queue, plus the memory used in the receive queue is greater than the socket receive buffer size, then the RcvBufferErrors and InErrors metrics are updated in the udpstats table, along with the socket's drop count.</p>
<p>To <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1594">safely handle multi-threaded access to the socket buffer</a>, if an application is currently reading from a socket, then the inbound packet will be queued to the socket's <a href="http://lxr.free-electrons.com/source/include/net/sock.h?v=4.0#L335">backlog queue</a>. The backlog queue will be processed during the handling of the next sortIRQ event.</p>
<p>Otherwise, pending more checks that socket memory limits have not be exceeded, the packet is added to the socket's <a href="http://lxr.free-electrons.com/source/net/core/sock.c?v=4.0#L439"><code>sk_receive_queue</code></a>.</p>
<p>Finally, once the packet has been delivered to the socket's receive queue, notification is <a href="http://lxr.free-electrons.com/source/net/core/sock.c?v=4.0#L474">sent to any interested listeners</a>.</p>
<h2 id="kernel-statistics">Kernel statistics</h2>
<p>Now that we have seen where the various capacity checks are made, we can make some observations in the <code>proc</code> file system.</p>
<p>There are protocol-specific files in <code>/proc/net</code> that are updated by the kernel whenever data is received.</p>
<h3 id="procnetsnmp"><code>/proc/net/snmp</code></h3>
<p>This file contains statistics on multiple protocols, and is the target for updates to the UDP receive buffer errors. So when we want to know about system-wide receive errors, this is the place to look. When this file is read, a <a href="http://lxr.free-electrons.com/source/net/ipv4/proc.c?v=4.0#L374">function call</a> iterates through the stored values and writes them to the supplied destination. Such is the magic of <a href="https://en.wikipedia.org/wiki/Procfs"><code>procfs</code></a>.</p>
<p>In the output below, we can see that there have been several receive errors (<code>RcvbufErrors</code>), and the number is matched by the <code>InErrors</code> metric:</p>
<pre><code>Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors InCsumErrors
Udp: 281604018683 4      261929024 42204463516 261929024    0            0</code></pre>
<p>Each of the columns maps to a constant specified <a href="http://lxr.free-electrons.com/source/net/ipv4/proc.c?v=4.0#L176">here</a>, so to determine when these values are updated, we simply need to <a href="https://www.google.co.uk/search?q=UDP_MIB_RCVBUFERRORS&amp;sitesearch=lxr.free-electrons.com/source">search for the constant name</a>.</p>
<p>There is a subtle difference between <code>InErrors</code> and <code>RcvbufErrors</code>, but in our case when looking for socket buffer overflow, we only care about <code>RcvbufErrors</code>.</p>
<h3 id="procnetudp"><code>/proc/net/udp</code></h3>
<p>This file contains socket-specific statistics, which only live for the lifetime of a socket. As we have seen from the code so far, each time the <code>RcvbufErrors</code> metric has been incremented, so too has the corresponding <code>sk_drops</code> value for the target socket.</p>
<p>So if we need to differentiate between drop events on a per-socket basis, this file is what we need to observe. The contents below can be explained by looking at the <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c#L2517">handler function</a> for socket data:</p>
<pre><code>[pricem@metal ~]# cat /proc/net/udp
sl  local_address rem_address   st tx_queue rx_queue tr tm-&gt;when retrnsmt   uid  timeout inode ref pointer drops
47: 00000000:B87A 00000000:0000 07 00000000:00000000 00:00000000 00000000   512        0 239982339 2 ffff88052885c440 0     
95: 4104C1EF:38AA 00000000:0000 07 00000000:00000000 00:00000000 00000000   512        0 239994712 2 ffff8808507ed800 0     
95: BC04C1EF:38AA 00000000:0000 07 00000000:00000000 00:00000000 00000000   512        0 175113818 2 ffff881054a8b080 0     
95: BE04C1EF:38AA 00000000:0000 07 00000000:00000000 00:00000000 00000000   512        0 175113817 2 ffff881054a8b440 0     
...</code></pre>
<p>For observing back-pressure, we are interested in the drops column, and the rx_queue column, which is a snapshot of the amount of queued data at the time of reading. Local and remote addresses are hex-encoded <code>ip:port</code> addresses.</p>
<p>Monitoring this file can tell us if our application is falling behind the packet arrival rate (increase in <code>rx_queue</code>), or whether the kernel was unable to copy a packet into the socket buffer due to it already being full (increase in <code>drops</code>).</p>
<h2 id="tracepoints">Tracepoints</h2>
<p><code>/proc/net/udp</code>: http://lxr.free-electrons.com/source/net/ipv4/udp.c#L2538</p>
<p><code>/proc/net/snmp</code>: http://lxr.free-electrons.com/source/net/ipv4/proc.c?v=4.0#L176</p>
<p>Diff between InErrors and RevbufErrors:</p>
<p>Udp: InDatagrams NoPorts InErrors OutDatagrams RcvbufErrors SndbufErrors InCsumErrors Udp: 275707389845 4 261757187 41217478199 261757187 0 0</p>
<p>InErrors &gt; RcvbufErrors -&gt; allocation failure when cloning skb.http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1626</p>
<p>TODO: how does /sys/class/net/p3p1/statistics/rx_dropped get populated by the kernel? TODO: double-check the code that updates device-side drops/rx-ring overflow.</p>
