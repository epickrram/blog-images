<h1 id="navigating-the-linux-kernel-network-stack-receive-path">Navigating the Linux kernel network stack: receive path</h1>
<h2 id="background">Background</h2>
<p>At <a href="https://lmax.com">work</a> we practice continuous integration in terms of <a href="http://epickrram.blogspot.co.uk/2014/05/performance-testing-at-lmax-part-one.html">performance testing</a> alongside different stages of functional testing.</p>
<p>In order to do this, we have a performance environment that fully replicates the hardware and software used in our production environments. This is necessary in order to be able to find the limits of our system in terms of throughput and latency, and means that we make sure that the environments are identical, right down to the network cables.</p>
<p>Since we like to be ahead of the curve, we are constantly trying to push the boundaries of our system to find out where it will fall over, and the nature of the failure mode.</p>
<p>This involves <a href="http://epickrram.blogspot.co.uk/2014/08/performance-testing-at-lmax-part-three.html">running production-like load</a> against the system at a much higher rate than we have ever seen in production. We currently aim to be able to handle a constant throughput of 2-5 times the maximum peak throughput ever observed in production. We believe that this will give us enough headroom to handle future capacity requirements.</p>
<p>Our Performance &amp; Capacity team has a constant background task of:</p>
<ol style="list-style-type: decimal">
<li>increase load applied to the system until it breaks</li>
<li>find and fix the bottleneck</li>
</ol>
<p>Using this process, we aim to ensure that we are able to handle spikes in demand, and increases in user numbers, while still achieving a consistent and low latency-profile.</p>
<p>There is actually a third step to this process, which is something like 'buy new hardware' or 'modify the system to do less work', since there is only so much than tuning will buy you.</p>
<h2 id="identifying-a-bottleneck">Identifying a bottleneck</h2>
<p>During the latest iteration of the break/fix cycle, we identified one particular service as problematic. The service in question is one of those responsible for consuming the output of our matching engine, which can peak at 250,000 messages per second at our current performance-test load.</p>
<p>When we have a bottleneck in one of our services, it usually follows a familiar pattern of back-pressure from the application processing thread, resulting in packets being dropped by the networking card.</p>
<p>In this case however, we could see from our monitoring that we were not suffering from any processing back-pressure, so it was necessary to delve a little deeper into the network packet receive path in order to understand the problem.</p>
<h2 id="understanding-the-data-flow">Understanding the data flow</h2>
<p>The Linux kernel provides a number of counters that can give an indication of any problems in the network stack. Since we are concerned with throughput, we will be most interested in things like queue depths and drop counts.</p>
<p>Before looking at the available statistics, let's take a look at how a packet is handled once it is pulled off the wire.</p>
<p>The journey begins in the network driver code; this is vendor-specific and in the majority of cases open source. In this example, we're working with an Intel 10Gb card, which uses the ixgbe driver. You can find out the driver used by a network interface by using <code>ethtool</code>:</p>
<pre><code>ethtool -i &lt;device-name&gt;</code></pre>
<p>This will generate output that looks something like:</p>
<pre><code>driver: ixgbe
version: 3.19.1-k
firmware-version: 0x546d0001
bus-info: 0000:41:00.0
supports-statistics: yes
supports-test: yes
supports-eeprom-access: yes
supports-register-dump: yes
supports-priv-flags: no</code></pre>
<p>The driver code can be found in the Linux kernel source <a href="http://lxr.free-electrons.com/source/drivers/net/ethernet/intel/ixgbe/?v=4.0">here</a>.</p>
<h3 id="napi">NAPI</h3>
<p>NAPI, or New API is a mechanism introduced into the kernel several years ago. More background can be read <a href="http://www.linuxfoundation.org/collaborate/workgroups/networking/napi">here</a>, but in summary, NAPI increases network receive performance by changing packet receipt from interrupt-driven to polling-mode.</p>
<p>Previous to the introduction of NAPI, network cards would typically fire a hardware interrupt for each received packet. Since an interrupt on a CPU will always cause suspension of the executing software, a high interrupt rate can interfere with software performance. NAPI addresses this by exposing a poll method to the kernel, which is periodically executed (actually via an interrupt). While the poll method is executing, receive interrupts for the network device are disabled. The effect of this is that the kernel can drain potentially multiple packets from the network device receive buffer, thus increasing throughput at the same time as reducing the interrupt overhead.</p>
<h3 id="interrupt-handling">Interrupt handling</h3>
<p>When the network device driver is initially configured, it first associates a handler function with the receive interrupt. This function will be invoked whenever the CPU receives a hardware interrupt from the network card. For the card that we're looking at, this happens in a method called <a href="http://lxr.free-electrons.com/source/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c?v=4.0#L2740">ixgbe_request_msix_irqs</a>:</p>
<pre><code>request_irq(entry-&gt;vector, &amp;ixgbe_msix_clean_rings, 0,
   q_vector-&gt;name, q_vector);</code></pre>
<p>So when an interrupt is received by the CPU, the ixgbe_msix_clean_rings method simply <a href="http://lxr.free-electrons.com/source/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c?v=4.0#L2675">schedules a NAPI poll</a>, and returns IRQ_HANDLED:</p>
<pre><code>static irqreturn_t ixgbe_msix_clean_rings(int irq, void *data)
{
    struct ixgbe_q_vector *q_vector = data;
    ...
    if (q_vector-&gt;rx.ring || q_vector-&gt;tx.ring)
        napi_schedule(&amp;q_vector-&gt;napi);

    return IRQ_HANDLED;
}</code></pre>
<p>Scheduling the NAPI poll entails <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L3022">adding some work</a> to the per-cpu poll list maintained in the <code>softnet_data</code> structure:</p>
<pre><code>static inline void ____napi_schedule(struct softnet_data *sd,
                                     struct napi_struct *napi)
{
    list_add_tail(&amp;napi-&gt;poll_list, &amp;sd-&gt;poll_list);
    __raise_softirq_irqoff(NET_RX_SOFTIRQ);
}</code></pre>
<p>and then raising a <code>softirq</code> event. Once the <code>softirq</code> event has been raised, the driver knows that the poll function will be called in the near future.</p>
<h3 id="softirq-processing">softirq processing</h3>
<p>For more background on interrupt handling, the <a href="https://lwn.net/Kernel/LDD3/">Linux Device Drivers</a> book has a chapter dedicated to this topic. Suffice to say, doing work inside of a hardware interrupt context is generally avoided within the kernel; while handling a hardware interrupt a CPU is not executing user or kernel software threads, and no other hardware interrupts can be handled until the current routine is complete. One mechanism for dealing with this is to use <code>softirqs</code>.</p>
<p>Each CPU in the system has a bound process called <code>ksoftirqd/&lt;cpu_number&gt;</code>, which is responsible for processing <code>softirq</code> events.</p>
<p>In this manner, when a hardware interrupt is received, the driver raises a softIRQ to be processed on the <code>ksoftirqd</code> process. So it is this process that will be responsible for calling the driver's <code>poll</code> method.</p>
<p>The <code>softirq</code> handler <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L7475"><code>net_rx_action</code></a> is configured for network packet receive events during device initialisation. All <code>softirq</code> events of type <code>NET_RX_SOFTIRQ</code> will be handled by the <code>net_rx_action</code> function.</p>
<p>So, having followed the code this far, we can say that when a network packet is in the device's receive ring-buffer, the <code>net_rx_action</code> function will be the top-level entry point for packet processing.</p>
<h3 id="net_rx_action">net_rx_action</h3>
<p>At this point, it is instructive to look at a function trace of the <code>ksoftirqd</code> process. This trace was generated using <a href="https://www.kernel.org/doc/Documentation/trace/ftrace.txt"><code>ftrace</code></a>, and gives a high-level overview of the functions involved in processing the available packets on the network device.</p>
<pre><code>net_rx_action() {
  ixgbe_poll() {
    ixgbe_clean_tx_irq();
    ixgbe_clean_rx_irq() {
      ixgbe_fetch_rx_buffer() {
        ... // allocate buffer for packet
      } // returns the buffer containing packet data
      ... // housekeeping
      napi_gro_receive() {
        // generic receive offload
        dev_gro_receive() {
          inet_gro_receive() {
            udp4_gro_receive() {
              udp_gro_receive();
            }
          }
        }
        netif_receive_skb_internal() {
          __netif_receive_skb() {
            __netif_receive_skb_core() {
              ...
              ip_rcv() {
                ...
                ip_rcv_finish() {
                  ...
                  ip_local_deliver() {
                    ip_local_deliver_finish() {
                      raw_local_deliver();
                      udp_rcv() {
                        __udp4_lib_rcv() {
                          __udp4_lib_mcast_deliver() {
                            ...
                            // clone skb &amp; deliver
                            flush_stack() {
                              udp_queue_rcv_skb() {
                                ... // data preparation
                                // [deliver UDP packet](http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1497)
                                // http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1497
                                // check if buffer is full
                                // http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1584
                                __udp_queue_rcv_skb() {
                                  // deliver to socket queue
                                  // http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1453
                                  // check for delivery error
                                  // http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1464
                                  sock_queue_rcv_skb() {
                                    ...
                                    _raw_spin_lock_irqsave();
                                    // enqueue packet to socket buffer list
                                    // http://lxr.free-electrons.com/source/include/linux/skbuff.h?v=4.0#L1481
                                    _raw_spin_unlock_irqrestore();
                                    // wake up listeners
                                    // http://lxr.free-electrons.com/source/net/core/sock.c?v=4.0#L474
                                    sock_def_readable() {
                                      __wake_up_sync_key() {
                                        _raw_spin_lock_irqsave();
                                        __wake_up_common() {
                                          ep_poll_callback() {
                                            ...
                                            _raw_spin_unlock_irqrestore();
                                          }
                                        }
                                        _raw_spin_unlock_irqrestore();
                                      }
...</code></pre>
<p>The <code>softirq</code> handler performs the following steps:</p>
<ol style="list-style-type: decimal">
<li>Call the driver's poll method (in this case <code>ixgbe_poll</code>)</li>
<li>Perform some <a href="https://lwn.net/Articles/358910/">GRO</a> functions to group packets together into a larger work unit</li>
<li>Call the packet type's <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L1735">handler function</a> (<code>ip_rcv</code>) to walk down the protocol chain</li>
<li>Parse IP headers, perform checksumming then call <code>ip_rcv_finish</code></li>
<li>The buffer's destination function is invoked, in this case <code>udp_rcv</code></li>
<li>Since these are multicast packets, <code>__udp4_lib_mcast_deliver</code> is called</li>
<li>The packet is copied and delivered to each registered UDP socket queue</li>
<li>In <code>udp_queue_rcv_skb</code>, buffers are checked and if space remains, the skb is added to the end of the socket's queue</li>
</ol>
<h2 id="monitoring-back-pressure">Monitoring back-pressure</h2>
<p>When attempting to increase the throughput of an application, we need to understand where back-pressure is coming from.</p>
<p>At this point in the data receive path, we could have throughput issues for two reasons:</p>
<ol style="list-style-type: decimal">
<li>The <code>softirq</code> handling mechanism cannot dequeue packets from the network device fast enough</li>
<li>The application processing the destination socket is not dequeuing packets from the socket buffer fast enough</li>
</ol>
<h3 id="softirq-back-pressure">softirq back-pressure</h3>
<p>For the first case, we need to look at softnet stats (<code>/proc/net/softnet_stat</code>), which are maintained by the network receive stack.</p>
<p>The softnet stats are defined <a href="http://lxr.free-electrons.com/source/include/linux/netdevice.h?v=4.0#L2444">here</a> as the per-cpu struct <code>softnet_data</code>, which contains a few fields of interest: <code>processed</code>, <code>time_squeeze</code> and <code>dropped</code>.</p>
<p><code>processed</code> is the total number of packets processed, so is a good indicator of total throughput.</p>
<p><code>time_squeeze</code> is updated if the <code>ksoftirq</code> process cannot process all packets available in the network device ring-buffer before its cpu-time is up. The process is limited to 2 jiffies of processing time, or a certain amount of 'work'. There are a couple of sysctls that control these parameters:</p>
<ol style="list-style-type: decimal">
<li><code>net.core.netdev_budget</code> - the total amount of processing to be done in one invocation of net_rx_action</li>
<li><code>net.core.dev_weight</code> - an indicator to the network driver of how much work to do per invocation of its napi poll method</li>
</ol>
<p>The <code>ksoftirq</code> daemon will continue to <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L4655">call <code>napi_poll</code></a> until either the time has run out, or the amount of work reported as completed by the driver exceeds the value of <code>net.core.netdev_budget</code>.</p>
<p>This behaviour will be driver-specific; in the Intel 10Gb driver, completed work will always be <a href="http://lxr.free-electrons.com/source/drivers/net/ethernet/intel/ixgbe/ixgbe_main.c?v=4.0#L2727">reported as <code>net.core.dev_weight</code></a> if there are still packets to be processed at the end of a poll invocation.</p>
<p>Given some example numbers, we can determine how many times the napi_poll function will be called for a <code>softIRQ</code> event:</p>
<pre><code>net.core.netdev_budget = 300
net.core.dev_weight = 64

poll_count = (300 / 64) + 1 =&gt; 5</code></pre>
<p>If there are still packets to be processed in the network device ring-buffer, then the <code>time_squeeze</code> counter will be incremented for the given CPU.</p>
<p>The <code>dropped</code> counter is only used when the <code>ksoftirq</code> process is attemping to add a packet to the backlog queue of another CPU. This can happen if <a href="https://www.kernel.org/doc/Documentation/networking/scaling.txt">Receive Packet Steering</a> is enabled, but since we are only looking at UDP multicast without RPS, I won't go into the detail.</p>
<p>So if our kernel helper thread is unable to move packets from the network device receive queue to the socket's receive buffer fast enough, we can expect the <code>time_squeeze</code> column in <code>/proc/net/softnet_stat</code> to increase over time.</p>
<p>In order to interpret the file, it is worth looking at <a href="http://lxr.free-electrons.com/source/net/core/net-procfs.c?v=4.0#L146">the implementation</a>. Each row represents a CPU-local instance of the <code>softnet_stat</code> struct (starting with CPU0 at the top), and the third column is the <code>time_squeeze</code> entry.</p>
<p>The only tunable that we have at our disposal is the <code>netdev_budget</code> value. Increasing this will allow the <code>ksoftirq</code> process to do more work. The process will still be limited by a total processing time of 2 jiffies though, so there will be an upper ceiling to packet throughput.</p>
<p>Given the speeds that modern processors are capable of, it is unlikely that the <code>ksoftirq</code> daemon will be unable to keep up with the flow of data. In order to give the kernel the best chance to do so, make sure that there is no contention for CPU resources by assigning network interrupts to a number of cores, and then using isolcpus to make sure that no other processes will be running on them. This will give the <code>ksoftirq</code> daemon the best chance of copying the inbound packets in a timely manner.</p>
<p>If the <code>ksoftirq</code> daemon is squeezed frequently enough, or is just unable to get CPU time, then the network device will be forced to drop packets from the wire. In this case, we can use ethtool to find the rx_missed_errors count:</p>
<pre><code>ethtool -S &lt;device-name&gt; | grep rx_missed
rx_missed_errors: 0</code></pre>
<p>alternatively, the same data can be found by looking at the following file:</p>
<pre><code>/sys/class/net/&lt;device-name&gt;/statistics/rx_missed_errors</code></pre>
<p>For a full description of each of the statistics reported by <code>ethtool</code>, refer to <a href="http://lxr.free-electrons.com/source/Documentation/ABI/testing/sysfs-class-net-statistics">this document</a>.</p>
<h3 id="application-back-pressure">Application back-pressure</h3>
<p>It is far more likely that our user programs will be the bottleneck here, and in order to determine whether that is the case, we need to look at the next stage in the message receipt path. A continuation of this post will explore that area in more detail.</p>
<h2 id="summary">Summary</h2>
<p>For UDP-multicast traffic, we have seen in detail the code paths involved in moving an inbound network packet from a network device to a socket's input buffer. This stage can be broadly summarised as follows:</p>
<ol style="list-style-type: decimal">
<li>On packet receipt, the network device fires a hardware interrupt to the configured CPU</li>
<li>The hardware interrupt handler schedules a <code>softIRQ</code> on the same CPU</li>
<li>The <code>softIRQ</code> handler thread (<code>ksoftirqd</code>) will disable receive interrupts and poll the card for received data</li>
<li>Data will be copied from the network device's receive buffer into the destination socket's input buffer</li>
<li>After a certain amount of work has been done, or no inbound packets remain, the <code>ksoftirq</code> daemon will re-enable receive interrupts and return</li>
</ol>
<p>In order to optimise for throughput, there are a couple of things to try tuning:</p>
<ol style="list-style-type: decimal">
<li>Increase the amount of work that the <code>ksoftirq</code> daemon is allowed to do (<code>net.core.netdev_budget</code>)</li>
<li>Make sure that the <code>ksoftirq</code> daemon is not contending for CPU resource or being descheduled due to other hardware interrupts</li>
<li>Increase the size of the network device's ring-buffer (<code>ethtool -g &lt;device-name&gt;</code>)</li>
</ol>
<p>As with all performance-related experiments, never attempt to tune the system without being able to measure the impact of any changes in isolation.</p>
<p>First, make sure that you know what the problem is (i.e. <code>rx_missed_errors</code> or <code>time_squeeze</code> is increasing), the add the relevant monitoring. For this particular case, we would want to be able to correlate the application experiencing message loss with a change in the relevant counters, so recording and charting the numbers would be a good start.</p>
<p>Once this has been done, changes can be made to system configuration to see if an improvement can be made.</p>
<p>Lastly, any changes to the tuning parameters that I've mentioned MUST be configured via automation. We have sadly lost a fair amount of time to manual changes being made on machines that have not persisted across reboots.</p>
<p>It is all too easy (and I speak as a repeat offender) to make adjustments, find the optimal configuration, and then move on to something else. Do yourself and your colleagues a favour and automate configuration management!</p>
<p>__netif_receive_skb_core increments softnetdata.processed (http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L3646) ip_rcv updates received, discarded, checksum errors, truncated packets (http://lxr.free-electrons.com/source/net/ipv4/ip_input.c?v=4.0#L363) flush_stack updates sock net rcvbuff, input errors (http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1628) udp_queue_rcv_skb updates rcvbuff errors on sock net (http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1585)</p>
