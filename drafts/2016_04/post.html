<h1 id="navigating-the-linux-kernel-network-stack-into-user-land">Navigating the Linux kernel network stack: into user land</h1>
<p>This is a continuation of my <a href="http://epickrram.blogspot.co.uk/2016/05/navigating-linux-kernel-network-stack.html">previous post</a> in which we follow the path of an inbound multicast packet to the user application.</p>
<p>At the end of the last post, I mentioned that application back-pressure was likely to be the cause of receiver packet-loss. As we continue through the code path taken by inbound packets up into user-land, we will see the various points at which data will be discarded due to slow application processing, and what metrics can shed light on these events.</p>
<h2 id="protocol-mapping">Protocol mapping</h2>
<p>First of all, let's take a quick look at how a received data packet is matched up to its <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L1735">handler function</a>.</p>
<p>We can see from the stack-trace below that the top-level function calls when dealing with the packets are <code>packet_rcv</code> and <code>ip_rcv</code>.</p>
<pre><code>__netif_receive_skb_core() {
    packet_rcv() {
        skb_push();
        __bpf_prog_run();
        consume_skb();
    }
    bond_handle_frame() {
        bond_3ad_lacpdu_recv();
    }
    packet_rcv() {
        skb_push();
        __bpf_prog_run();
        consume_skb();
    }
    packet_rcv() {
        skb_push();
        __bpf_prog_run();
        consume_skb();
    }
    ip_rcv() {
        ...</code></pre>
<p>This trace captures the <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L3671">part of the receive path</a> where the inbound packet is passed to each registered handler function. In this way, the kernel handles things like VLAN tagging, interface bonding, and packet-capture. Note the <code>__bpf_prog_run</code> function call, which indicates that this is the entry point for <code>pcap</code> packet capture and filtering.</p>
<p>The protocol-to-handler mapping can be viewed in the file <code>/proc/net/ptype</code>:</p>
<pre><code>[pricem@metal]$ cat /proc/net/ptype 
Type Device      Function
ALL  em1      packet_rcv
0800          ip_rcv
0011          llc_rcv [llc]
0004          llc_rcv [llc]
88f5          mrp_rcv [mrp]
0806          arp_rcv
86dd          ipv6_rcv</code></pre>
<p>Comparing against <a href="https://en.wikipedia.org/wiki/EtherType#Notable_values">this reference</a>, it is clear that the kernel reads the value of the ethernet frame's <code>EtherType</code> field and dispatches to the corresponding function. There are also some functions that will be executed for all packet types (signified by type <code>ALL</code>).</p>
<p>So our inbound packet will be processed by the capture hooks, even if no packet capture is running (this then must presumably be very cheap), before being passed to the correct protocol handler, in this case <code>ip_rcv</code>.</p>
<h2 id="the-socket-buffer">The socket buffer</h2>
<p>As discussed previously, each socket on the system has a receive-side FIFO queue that is written to by the kernel, and read from by the user application.</p>
<p><a href="http://lxr.free-electrons.com/source/net/ipv4/ip_input.c?v=4.0#L376"><code>ip_rcv</code></a> starts by getting its own copy of the incoming packet, copying if the packet is already shared. If the copy fails due to lack of memory, then the packet is discarded, and the <code>Discard</code> count of ipstats is incremented. Other checks made at this point include the IP checksum, header-length check and truncation check, each of which update the relevant metrics in the ipstats table.</p>
<p>Before calling the <code>ip_rcv_finish</code> function, the packet is diverted through the <code>netfilter</code> module where software-based network filtering can be applied.</p>
<p>Assuming that the packet is not dropped by a filter, <a href="http://lxr.free-electrons.com/source/net/ipv4/ip_input.c?v=4.0#L312"><code>ip_rcv_finish</code></a> passes the packet on to the next protocol-handler in the chain.</p>
<p>In the internals of the <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1749"><code>udp_rcv</code></a> function, we finally get to the point of accessing the socket FIFO queue.</p>
<p>Simple validation performed at this point includes packet-length check and checksum. Failure of these checks will cause the relevant statistics to be updated in the udpstats table.</p>
<p>Because the traffic we're tracing <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1801">is multicast</a>, the next handler function is <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1660"><code>__udp4_lib_mcast_deliver</code></a>.</p>
<p>With some exotic locking in place, the kernel determines how many different sockets this multicast packet needs to be delivered to. It is worth noting here that there is a smallish number (<code>256 / sizeof( struct sock)</code>) of sockets that can be listening to a given multicast address before the possibility of lock-contention creeps in.</p>
<p>An effort is made to enumerate the registered sockets with a lock held, then perform the packet dispatching without a lock. However, if the number of registered sockets exceeds the specified threshold, then packet dispatch (the <code>flush_stack</code> function) will be handled while a lock is held.</p>
<p>Once in <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1614"><code>flush_stack</code></a>, the packet is copied for each registered socket, and pushed onto the FIFO queue.</p>
<p>If the kernel is unable to allocate memory in order to copy the buffer, the socket's drop-count will be incremented, along with the RcvBufErrors and InErrors metrics in the udpstats table.</p>
<p>After another checksum test, we are finally at the point where socket-buffer overflow <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1584">is tested</a>:</p>
<pre><code>if (sk_rcvqueues_full(sk, sk-&gt;sk_rcvbuf)) {
    UDP_INC_STATS_BH(sock_net(sk), UDP_MIB_RCVBUFERRORS,
                     is_udplite);
    goto drop;
}</code></pre>
<p>If the socket's backlog queue, plus the already-allocated memory is greater than the socket receive buffer size, then the RcvBufferErrors and InErrors metrics are updated in the udpstats table, along with the socket's drop count.</p>
<p>To <a href="http://lxr.free-electrons.com/source/net/ipv4/udp.c?v=4.0#L1594">safely handle multi-threaded access to the socket buffer</a>, if an application is currently reading from a socket, then the inbound packet will be queued to the socket's backlog queue.</p>
<p>Otherwise, pending more checks that socket memory limits have not be exceeded, the packet is added to the socket's <a href="http://lxr.free-electrons.com/source/net/core/sock.c?v=4.0#L439"><code>sk_receive_queue</code></a>.</p>
<p>Finally, once the packet has been delivered to the socket's receive queue, notification is <a href="http://lxr.free-electrons.com/source/net/core/sock.c?v=4.0#L474">sent to any interested listeners</a>.</p>
<p>As covered previously, the data structure containing information about network-related softIRQ processing on each CPU is <code>softnet_data</code>, exposed in the <code>proc</code> filesystem as <code>/proc/net/softnet_stat</code>.</p>
<p>In the receive loop of <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L3619"><code>__netif_receive_skb_core</code></a>, we can see the <code>processed</code> count <a href="http://lxr.free-electrons.com/source/net/core/dev.c?v=4.0#L3646">being incremented</a> for each socket buffer handled:</p>
<pre><code>__this_cpu_inc(softnet_data.processed);</code></pre>
<p>So this is the first column in the <code>softnet_stat</code> file.</p>
<p>/proc/net/snmp: http://lxr.free-electrons.com/source/net/ipv4/ip_input.c?v=4.0#L388</p>
<p>http://lxr.free-electrons.com/source/net/ipv4/proc.c?v=4.0#L374</p>
<p>[pricem@ares buck-all]$ cat /proc/net/ptype Type Device Function ALL em1 packet_rcv 0800 ip_rcv 0011 llc_rcv [llc] 0004 llc_rcv [llc] 88f5 mrp_rcv [mrp] 0806 arp_rcv 86dd ipv6_rcv</p>
